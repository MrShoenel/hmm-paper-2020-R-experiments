---
title: "R Notebook"
output: html_notebook
---

```{r}
source("../models/neural.R")
```

# Testing

We want to test our model `__m1__` using the weighted RSS loss (_wRSS_). The test is simple: try to learn XOR. We use a network with 4 hidden, relu-activated units. The output is sigmoid-activated.

```{r}
set.seed(0xbeef)

lxl <- 4
wh_xor <- glorot_weights(2,lxl)
bh_xor <- glorot_weights(1,lxl)
wo_xor <- glorot_weights(lxl,1)
bo_xor <- glorot_weights(1,1)

XY_xor <- matrix(data = c(0,0,1,1,0,1,0,1,0,1,1,0), nrow = 4)
XY_xor
```

Let's train the network!

```{r}
xor_weights <- gradient_descent_m1(
  X=XY_xor[,1:2], Y=matrix(data = XY_xor[,3], nrow = 4),
  w_h_0 = wh_xor, b_h_0 = bh_xor, w_o_0 = wo_xor, b_o_0 = bo_xor,
  epochs = 1e4, learning_rate = 1e-1, precision = 1e-5,
  act.fn = relu, act.fn.derive = relu_d1,
  err.fn = RSS, err.fn.derive = RSS_d1)
```

```{r}
print(sapply(1:nrow(XY_xor), function(rn) {
  loss_wRSS_m1(
    X = matrix(data = XY_xor[rn, 1:2], nrow = 1),
    Y = matrix(data = XY_xor[rn ,3], nrow = 1),
    w_h = xor_weights$w_h, b_h = xor_weights$b_h,
    w_o = xor_weights$w_o, b_o = xor_weights$b_o,
    act.fn = relu, act.fn.derive = relu_d1,
    err.fn = RSS)
}))

print(XY_xor)

print(sapply(1:nrow(XY_xor), function(rn) {
  m1(x_i = XY_xor[rn, 1:2],
    w_h = xor_weights$w_h, b_h = xor_weights$b_h,
    w_o = xor_weights$w_o, b_o = xor_weights$b_o,
    act.fn = relu, act.fn.derive = relu_d1)
}))
```

Number of elapsed epochs:
```{r}
length(xor_weights$hist_loss)
```


```{r}
plot(x = 1:length(xor_weights$hist_loss), y = xor_weights$hist_loss)
```


.. and plot a perspective plot:

```{r}
m1_xor_wrap <- function(x1, x2, w_h, b_h, w_o, b_o) {
  N <- length(x1)
  z <- rep(0, N)
  for (i in 1:N) {
    z[i] <- m1(
      x_i = c(x1[i], x2[i]),
      w_h = w_h, b_h = b_h, w_o = w_o, b_o = b_o)
  }
  z
}

x <- y <- seq(0,1,by = 0.05)
z <- outer(x, y, function(x1,x2) {
  m1_xor_wrap(
    x1, x2,
    w_h = xor_weights$w_h, b_h = xor_weights$b_h,
    w_o = xor_weights$w_o, b_o = xor_weights$b_o)
})

persp(x, y, z,
      main="Perspective Plot of XOR",
      zlab = "m1 output",
      theta = 235, phi = 25,
      col = "springgreen", shade = 0.33)
```

```{r}
temp <- xor_weights
temp$hist_loss <- NULL
temp
```


# Commit Data

Let's attempt to learn some 1st-order commits.

```{r}
temp <- data2Densities_1stOrder(states = states, data = commits_t1[,], stateColumn = "label_t_0", split = 0.85, normalizePdfs = TRUE)

train <- temp$train[,]
train_Y_obsId <- train$obsId
train$obsId <- NULL
#train$y <- paste0(train$y_t1, train$y_t0)
#train <- balanceDatasetSmote(train, "y")
#train$y <- NULL

# It is utmost important that we order these columns!
# The lower the index, the earlier a predictor has to appear!
train_Y <- as.matrix(train[, sort(colnames(train)[grepAnyOrAll("^y_t", colnames(train))])])
train_est <- train[, colnames(train) %in% c("est_t0", "est_t1")]
nzv <- caret::nzv(train, names = TRUE, saveMetrics = TRUE)
train <- train[, !(nzv$zeroVar | nzv$nzv)]
train <- train[, !grepAnyOrAll(ignoreCols, colnames(train))]
train <- train[, !grepAnyOrAll(c("label", "est_t", "y_t"), colnames(train))]
train_X <- as.matrix(cbind(train[, grepAnyOrAll("^d_", colnames(train))], train_est))

valid <- temp$valid[, ]
valid_X <- as.matrix(valid[, colnames(train_X)[!grepAnyOrAll("^y_t", colnames(train_X))]])
valid_Y <- as.matrix(valid[, colnames(train_Y)])
valid_Y_obsId <- valid$obsId
```

Let's prepare some neural network for this:

```{r}
set.seed(0xbeef)

lHl <- 10
wh_c <- glorot_weights(ncol(train_X),lHl)
bh_c <- glorot_weights(1,lHl)
wo_c <- glorot_weights(lHl,2)
bo_c <- glorot_weights(1,2)

c_weights <- gradient_descent_m1(
  X = train_X, Y = train_Y,
  w_h_0 = wh_c, b_h_0 = bh_c, w_o_0 = wo_c, b_o_0 = bo_c,
  epochs = 1e4, learning_rate = 1e-3,
  precision = 1e-5, batch_size = 32,
  act.fn = swish, act.fn.derive = swish_d1)#,
  #valid_X = valid_X, valid_Y = valid_Y)
```

```{r}
plot(x = 1:length(c_weights$hist_loss), y = c_weights$hist_loss, type = "l")
```

```{r}
temp <- m1_predict(X = train_X, Y_actual = train_Y,
                   w_h = c1_weights$w_h, b_h = c1_weights$b_h,
                   w_o = c1_weights$w_o, b_o = c1_weights$b_o,
                   act.fn = swish, act.fn.derive = swish_d1)
# obsId(1), pred(2), sump(1), truth(2), loss(1)
#temp <- data.frame(cbind(valid_Y_obsId, round(temp[, 1:2], digits = 4), round(temp[, 1] + temp[, 2], digits = 4), valid_Y, round(temp[, 3], digits = 4)))

temp <- data.frame(
  obsId = train_Y_obsId,
  pred_t1 = round(temp[, 1], digits = 4),
  pred_t0 = round(temp[, 2], digits = 4),
  pred_err = round(temp[, 3], digits = 4),
  pred_sum = round(temp[, 1] + temp[, 2], digits = 4),
  dens_sum = round(apply(train_X, 1, sum), digits = 4),
  dens_prod = round(apply(train_X, 1, function(row) prod(row + 0.1)), digits = 5),
  y_t1 = train_Y[, 1],
  y_t0 = train_Y[, 2]
)

temp %>% arrange(obsId, -pred_sum)
```

## Test

Let's try to learn from these results. The first test is to concatenate all preditions for one sample (transformed using all estimators) and just use a one-hot activation.

```{r}
predAsOneHot <- matrix(nrow = length(unique(temp$obsId)), ncol = 49)
i <- 1
for (oId in unique(temp$obsId)) {
  data <- temp[temp$obsId == oId, ]
  idx11 <- which(data$y_t1 == 1 & data$y_t0 == 1)
  vec11 <- rep(0, 9)
  vec11[idx11] <- 1
  predAsOneHot[i, ] <- c(data$pred_t1, sd(data$pred_t1), data$pred_t0, sd(data$pred_t0), data$dens_sum, sd(data$dens_sum), data$dens_prod, sd(data$dens_prod), vec11)
  i <- i + 1
}
predAsOneHot <- data.frame(predAsOneHot) # so we get col-names
scalerPredAsOneHot <- caret::preProcess(predAsOneHot, method = c("center", "scale"))
predAsOneHot <- predict(scalerPredAsOneHot, predAsOneHot)
```
```{r}
set.seed(23)
nnet <- neuralnet::neuralnet(
  formula = formula(paste0(paste(paste0("X", 41:49), collapse = " + "), "~.")),
  data = predAsOneHot,
  hidden = c(8, 4),
  startweights = c(glorot_weights(40, 8), glorot_weights(8, 4)),
  #act.fct = function(x) x / (1 + exp(-x)),
  lifesign = "full",
  threshold = 5e-3,
  stepmax = 15e4 # usually takes 50k steps, allow up to 150k
)
```

Now we have another network that can be used as a voting scheme:

```{r}
plot(nnet)
```

In order to check our validation data, we need to make predictions of it with the first network, then transform these into the one-hot data and predict with the 2nd network.

```{r}
temp_valid <- m1_predict(X = valid_X, Y_actual = valid_Y,
                   w_h = c1_weights$w_h, b_h = c1_weights$b_h,
                   w_o = c1_weights$w_o, b_o = c1_weights$b_o,
                   act.fn = swish, act.fn.derive = swish_d1)

temp_valid <- data.frame(
  obsId = valid_Y_obsId,
  pred_t1 = round(temp_valid[, 1], digits = 4),
  pred_t0 = round(temp_valid[, 2], digits = 4),
  pred_err = round(temp_valid[, 3], digits = 4),
  pred_sum = round(temp_valid[, 1] + temp_valid[, 2], digits = 4),
  dens_sum = round(apply(valid_X, 1, sum), digits = 4),
  dens_prod = round(apply(valid_X, 1, function(row) prod(row + 0.1)), digits = 5),
  y_t1 = valid_Y[, 1],
  y_t0 = valid_Y[, 2]
)
```
```{r}
predAsOneHot_valid <- matrix(nrow = length(unique(temp_valid$obsId)), ncol = 50)
i <- 1
for (oId in unique(temp_valid$obsId)) {
  data <- temp_valid[temp_valid$obsId == oId, ]
  idx11 <- which(data$y_t1 == 1 & data$y_t0 == 1)
  vec11 <- rep(0, 9)
  vec11[idx11] <- 1
  predAsOneHot_valid[i, ] <- c(data$pred_t1, sd(data$pred_t1), data$pred_t0, sd(data$pred_t0), data$dens_sum, sd(data$dens_sum), data$dens_prod, sd(data$dens_prod), vec11, oId)
  i <- i + 1
}
predAsOneHot_valid <- data.frame(predAsOneHot_valid) # so we get col-names
predAsOneHot_valid <- predict(scalerPredAsOneHot, predAsOneHot_valid)
```
```{r}
pred_valid <- predict(nnet, newdata = predAsOneHot_valid[, paste0("X", 1:40)])
cbind(
  round(pred_valid, digits = 3),
  apply(pred_valid, 1, which.max),
  predAsOneHot_valid[, "X50"])
```







