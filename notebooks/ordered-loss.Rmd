---
title: "R Notebook"
output: html_notebook
---

```{r}
if (length(grep("notebooks$", getwd())) == 0) {
  stop("Wrong working directory; must be ./notebooks")
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
source("../helpers.R", echo = FALSE)
source("../models/conddens.R", echo = FALSE)
source("../models/depmix.R", echo = FALSE)
source("../models/neural.R", echo = FALSE)
source("../models/rssOrdered.R", echo = FALSE)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
olReqPkgs <- c(
  "pracma", "e1071", "neuralnet", "parallel", "doParallel", "doSNOW",
  "ranger", "knitr", "devtools", "caret", "mlbench", "DMwR", "filelock")
install.packagesCond(olReqPkgs)
for (pkg in olReqPkgs) { library.silent(pkg) }

# This package is for Bayesian tools:
if ("mmb" %in% rownames(installed.packages()) == FALSE) {
  remotes::install_github("MrShoenel/R-mmb@ac92c4c", subdir = "pkg/mmb")
}
library("mmb")
```




# Introduction

In this notebook, we evaluate all the voting schemes applicable to data represented as conditional densities. We will evaluate 1st-order joint conditional density (JCD) models, based on the $\boldsymbol f$-estimator. These estimators will be used in the $\boldsymbol D$-models. The difference between $\boldsymbol e$- and $\boldsymbol f$-estimators is, that the latter also include the data from their previous states.

The voting schemes we test here are applicable to all JCD models (A through D), but we will only do an extensive hyperparameter search using the D-models, as they represent an advancement over the C-models, which performed best thus far.

## Voting Schemes

Voting schemes are necessary, because we get a potential answer for every estimator we pass a validation sample through. The purpose of a voting scheme is to aggregate all answers from all estimators, and pick the best-matching estimator based on some heuristics, statistics or model we learn.

## Ordered Loss

For every set of training samples, we will train an extra neural network. However, prior results showed that we cannot train such a network with unweighted losses, such as RSS or MSE. Therefore, we have developed three new loss functions that put a weight on each output, according to the outputs' order. Only with such losses it is possible to train models for our use-case.



# Hyperparameters

We are evaluating a tremendous amount of hyperparameters, which requires some serious parallel computation. Below, we define the full grid.

```{r}
set.seed(0xc0de)
olHp <- expand.grid(
  # This is for the cross validation. All pseudo-random functionality
  # should use this.
  resampleSeed = as.integer(runif(3, max = 2**31)),
  dens.fct = c("ecdf", "1-ecdf", "epdf", "epdf_scaled"),
  # Whether to oversample the unbalanced data before training.
  oversample = c(TRUE, FALSE),
  useKeywords = c(TRUE, FALSE),
  # data is always scaled, but scaling for density(data) is conditional.
  # If keywords are included, they're added to data before any other steps,
  # hence if data is scaled, so are the keywords.
  # Note the explicit type "data" is not included here, because our tests
  # for ordered loss are not compatible with it.
  useDatatype = c("density", "both", "density_scaled", "both_scaled"),
  # How to compute the densities for each continuous random variable.
  # Since PDFs are not necessary in the codomain [0,1], we have an extra
  # option to use scaled PDFs.
  #batchSize = c(8, 16, 32, 64),
  batchSize = c(20, 46, 72),
  # There is only one hidden layer.
  #neurons = c(5, 10, 17, 30, 50),
  neurons = c(5, 15, 30),
  learningRate = c(0.05, 0.005, 0.0005),
  err.fct = c("RSS", "wRSS", "omeRSS", "ome2RSS"),
  #act.fct = c("Lrelu", "swish", "gelu"),
  act.fct = c("Lrelu", "swish"),
  epochs = c(1e4),
  # For early stopping we add these:
  precision = c(5e-3),
  patience = c(5)
)

# Attach an ID to each hyperparameter set:
olHp$ID <- 1:nrow(olHp)

write.csv(olHp, file = "../results/ordered-loss_hyperparams.csv")
```

Attach an ID to each set of hyperparameters - we need that because we will also add a filter at runtime as we're using many instances to compute this.

```{r}
# Insert filter here (do not edit hyperparameters!):
#olHp <- olHp[olHp$dens.fct == "ecdf", ]

# (Deterministically) Randomize samples:
set.seed(0xc0de)
olHp <- olHp[sample(rownames(olHp)), ]
```

As of above, we will compute a total of `r nrow(olHp)` sets of hyperparameters. For reasons of comparison, we will attempt fitting a Random forest to each of the used datasets and to each seed. However, the other hyperparameters do not matter for those, so that we will create a separate grid and also obtain the results separately.

```{r}
rfHp <- expand.grid(
  resampleSeed = unique(olHp$resampleSeed),
  useDatatype = unique(olHp$useDatatype),
  useKeywords = unique(olHp$useKeywords),
  oversample = unique(olHp$oversample)
)
```

This will require a total of `r nrow(rfHp)` computations. We will define a function that takes one set of hyperparameters and fits a Random forest on it, then using the validation data to obtain metrics. This function will then be used in a parallel loop for all sets of hyperparameters.

```{r}
fitRandomForest <- function(hp, data, dataKw, states, stateColumn) {
  # First, we create a joined label for the data.
  stateColumn_t_1 <- gsub("_t_0$", "_t_1", stateColumn)
  
  trainValid <- generateDataForHpOrderedLoss(hp, data, dataKw, states, stateColumn)
  train_X <- trainValid$train_X
  train_Y <- trainValid$train_Y
  valid_X <- trainValid$valid_X
  valid_Y <- trainValid$valid_Y
  
  # We have to differentiate between two cases, based on the data:
  # Either we get two actual labels in the data, "label_t_0", "label_t_1"
  # (if data only, these are characters), or we get two binary labels
  # that indicate whether the correct density functions were used for
  # each state, hence the labels are 1/0 for "y_t1", "y_t0".
  
  useOnlyData <- hp$useDatatype == "data"
  yCols <- if (useOnlyData) c(stateColumn, stateColumn_t_1) else c("y_t0", "y_t1")
  
  if (useOnlyData) {
    train_Y <- data.frame(
      Y = paste0(train_Y[[yCols[1]]], "__|__", train_Y[[yCols[2]]])
    )
    valid_Y <- data.frame(
      Y = paste0(valid_Y[[yCols[1]]], "__|__", valid_Y[[yCols[2]]])
    )
  }
}
```


```{r}
# Slightly over-commit to keep it busy
doWithParallelCluster(numCores = as.integer(1.15 * parallel::detectCores()), expr = {
  pb <- txtProgressBar(min=1, max=nrow(olHp), style=3)
  progress <- function(n) setTxtProgressBar(pb, n)
  
  #data <- get1stOrderCommits()
  data <- readRDS("./commits_t1.rds")
  for (cn in colnames(data)) {
    if (is.character(data[, cn])) {
      # We have to do this, as oversampling will throw errors otherwise.
      data[, cn] <- as.factor(data[, cn])
    }
  }
  
  kw_t0 <- detectCommitKeywords(data$Message_t_0)
  colnames(kw_t0) <- paste0(colnames(kw_t0), "_t_0")
  kw_t1 <- detectCommitKeywords(data$Message_t_1)
  colnames(kw_t1) <- paste0(colnames(kw_t1), "_t_1")
  dataKw <- data.frame(cbind(kw_t0, kw_t1))
  
  states = sort(levels(data$label_t_0))
  
  allResults <- tryCatch({
    foreach::foreach(
      hpIdx = rownames(olHp),
      .packages = olReqPkgs,
      .options.snow = list(progress = progress)
    ) %dopar% {
      hp <- olHp[hpIdx, ]
      hpId <- hp$ID
      
      result <- tryCatch({
        evaluateHpOrderedLoss(
          hp = hp, data = data, dataKw = dataKw, states = states, stateColumn = "label_t_0")
      }, error=function(cond) cond)
      
      hadError <- is.list(result) && "message" %in% names(result)
      
      if (hadError) {
        writeOrAppend.csv("../results/ordered-loss_error.csv", data = data.frame(
          message = result$message,
          trace = trimws(as.character(.traceback(result)))
        ))
      } else {
        # It worked, let's sort out the results. The result-types are as follows:
        # One/Three: type -> b/l/r
        # Two: type (no b/l/r)
        # Four: b/l/r (no type)
        # Results One/Two are NULL if the density was scaled.
        # Result Three is always present.
        # Result Four is present if the data was NOT overscaled.
        
        dfR1 <- NULL
        if (is.list(result$One)) {
          for (scheme in names(result$One)) {
            res_b <- result$One[[scheme]][["both"]]
            res_l <- result$One[[scheme]][["left"]]
            res_r <- result$One[[scheme]][["right"]]
            
            # Add Id and type:
            res_all <- cbind(
              data.frame(hpId = hpId, scheme = scheme),
              res_b, res_l, res_r)
            # Add:
            dfR1 <- rbind(dfR1, res_all)
          }
          
          writeOrAppend.csv("../results/ordered-loss_R1.csv", data = dfR1)
        }
        
        
        dfR3 <- NULL
        if (is.list(result$Three)) {
          for (scheme in names(result$Three)) {
            res_b <- result$Three[[scheme]][["both"]]
            res_l <- result$Three[[scheme]][["left"]]
            res_r <- result$Three[[scheme]][["right"]]
            
            res_all <- cbind(
              data.frame(hpId = hpId, scheme = scheme),
              res_b, res_l, res_r)
            dfR3 <- rbind(dfR3, res_all)
          }
          
          writeOrAppend.csv("../results/ordered-loss_R3.csv", data = dfR3)
        }
        
        
        dfR2 <- NULL
        if (is.list(result$Two)) {
          for (scheme in names(result$Two)) {
            res_all <- cbind(
              data.frame(hpId = hpId, scheme = scheme),
              result$Two[[scheme]])
            dfR2 <- rbind(dfR2, res_all)
          }
          
          writeOrAppend.csv("../results/ordered-loss_R2.csv", data = dfR2)
        }
        
        
        dfR4 <- NULL
        if (is.list(result$Four)) {
          res_all <- data.frame(matrix(nrow = 1, ncol = 0))
          for (blr in names(result$Four)) {
            res_all <- cbind(res_all, result$Four[[blr]])
          }
          
          dfR4 <- cbind(
            data.frame(hpId = hpId), res_all)
          
          writeOrAppend.csv("../results/ordered-loss_R4.csv", data = dfR4)
        }
        
        
        dfMeta <- NULL
        if (is.list(result$meta)) {
          dfMeta <- cbind(
            data.frame(hpId = hpId),
            as.data.frame(result$meta))
          
          writeOrAppend.csv("../results/ordered-loss_meta.csv", data = dfMeta)
        }
      }
      
      # Finally return the raw result:
      temp <- list()
      temp[[hpId]] <- result
      temp
    }
  }, finally = {
    close(pb)
  })
  
  saveRDS(allResults, "allResults_ordered-loss.rds")
})
```




# Function used in Training

Now we are going to define one function that will take one set of hyperparameters and execute all evaluations neccessary, before returning all of the results. We get a lot of different results and will also write them to a specific CSV intermittently.

```{r}
# Mostly columns that could be used to identify a column. Note that
# we keep "label", as it'll be removed for inferencing only.
ignoreCols <- c(
  "project", # because of scarce data, we train and evaluate only cross-project
  "commitId",
  "labelledBefore",
  "reasonOrRule",
  "RepoPathOrUrl",
  "CommitterName",
  "CommitterTime",
  "CommitterEmail",
  "CommitterNominalLabel",
  "ParentCommitSHA1s",
  "Message",
  "branch",
  "SHA1",
  "AuthorName",
  "AuthorTime",
  "AuthorEmail",
  "AuthorNominalLabel"
)
```


```{r}
# doWithParallelCluster(numCores = 10, expr = {
#   pb <- txtProgressBar(min=1, max=nrow(rfHp), style=3)
#   progress <- function(n) setTxtProgressBar(pb, n)
#   
#   tryCatch({
#     foreach::foreach(
#       hpIdx = rownames(rfHp),
#       .packages = c("ranger"),
#       .options.snow = list(progress = progress)
#     ) %dopar% {
#       
#     }
#     
#   }, finally = {
#     close(pb)
#   })
# })
```





















