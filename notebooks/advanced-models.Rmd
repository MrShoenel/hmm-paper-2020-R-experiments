---
title: "Dependent Mixture Models and Joint Conditional Density Models"
output:
  md_document:
    df_print: paged
    toc: true
    toc_depth: 3
  pdf_document: 
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document: 
    df_print: kable
    fig_caption: yes
    keep_md: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
  word_document: 
    fig_caption: yes
    keep_md: yes
    toc: yes
  html_notebook: 
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
---




```{r warning=FALSE, echo=FALSE, message=FALSE}
source("../helpers.R")
install.packagesCond("knitr")
knitr::opts_chunk$set(rows.print=25, cols.print=15)
```

```{r}
source("../models/depmix.R")
source("../models/conddens.R")
```


# Overview and Notions

We use these notions:

* $\boldsymbol a$ -- a vector,
* $\boldsymbol A$ -- a matrix,
* $\mathsf A$ -- a tensor


# Preparation steps

## Data
We need to prepare 0th-, 1st-, and 2nd-order datasets, conditioned on the kind of preceding commit. There are extra functions to get those datasets.

```{r warning=FALSE, echo=FALSE, message=FALSE}
commits_t0_file <- normalizePath("../data/commits_t-0.csv", mustWork = FALSE)
if (file.exists(commits_t0_file)) {
  commits_t0 <- read.csv(commits_t0_file)
  commits_t0$X <- NULL
} else {
  commits_t0 <- getDataset("rq5_treex", removeUnwantedColums = FALSE)
  colnames(commits_t0) <- paste0(colnames(commits_t0), "_t_0")
  write.csv(commits_t0, file = commits_t0_file)
}


commits_t1_file <- normalizePath("../data/commits_t-1.csv", mustWork = FALSE)
if (file.exists(commits_t1_file)) {
  commits_t1 <- read.csv(commits_t1_file)
  commits_t1$X <- NULL
} else {
  commits_t1 <- get1stOrderCommits()
  write.csv(commits_t1, file = commits_t1_file)
}


commits_t2_file <- normalizePath("../data/commits_t-2.csv", mustWork = FALSE)
if (file.exists(commits_t2_file)) {
  commits_t2 <- read.csv(commits_t2_file)
  commits_t2$X <- NULL
} else {
  commits_t2 <- get2ndOrderCommits()
  write.csv(commits_t2, file = commits_t2_file)
}


commits_t3_file <- normalizePath("../data/commits_t-3.csv", mustWork = FALSE)
if (file.exists(commits_t3_file)) {
  commits_t3 <- read.csv(commits_t3_file)
  commits_t3$X <- NULL
} else {
  commits_t3 <- get3rdOrderCommits()
  write.csv(commits_t3, file = commits_t3_file)
}
```

## Packages

We will need a version of `mmb` for this, and we take an early but stable version by referring to a specific commit ID.

```{r warning=FALSE, echo=FALSE, message=FALSE}
install.packagesCond("devtools")
library("devtools")

# This package is for Bayesian tools:
if ("mmb" %in% rownames(installed.packages()) == FALSE) {
  remotes::install_github("MrShoenel/R-mmb@ac92c4c", subdir = "pkg/mmb")
}
library("mmb")
```


# Model Preparation

In this section, we will prepare initial state probability vectors/matrices and transition matrices/tensors. These are needed for the dependent mixture models. In the joint conditional density models, we only use initial state probabilities conditionally (we test with and without).

## Initial State Probabilities
When we define (1st-order) dependent mixture models, then there is a finite set of states $\mathcal{S} = \{a,c,p\}$ (corresponding to the maintenance activities of commits and summing to one), a vector $\boldsymbol \pi \in \mathcal{R}^{\left\lVert S \right\rVert}$ of initial state probabilities.

The intial probabilities are, regardless of the order of the model, always build from the $t-0$ (zeroth-order) commits, so we can always use the dataset `commits_t0`. In other words, each model, regardless of its order, uses the same $\phi_1(j)$ with the same $\boldsymbol \pi$, so that we only need to generate this once.

```{r}
initProbs <- c(
  a = nrow(commits_t0[commits_t0$label_t_0 == "a", ]),
  c = nrow(commits_t0[commits_t0$label_t_0 == "c", ]),
  p = nrow(commits_t0[commits_t0$label_t_0 == "p", ])
) / nrow(commits_t0)

print(initProbs)
```

## Transition Matrices and Tensors

With increasing order, models require tensors with higher order for their transitions. For a 1st-order model, this order is $2$ (a quadratic matrix actually). If more than one previous state is considered, then the matrix becomes a tensor of order $1$ + number of previous states considered ($\mathcal{R}^3$ for a 2nd-order model).

Every model's $\phi_2(j)$ depends on the likelihood of the current state and its previous state, hence we need transition probabilities. Hence, $\phi$'s order corresponds to the order of this tensor. So, for capturing transitions between two consecutive states, that order is two, and results in a matrix. For transitions between three states (a 2nd-order model), this becomes a 3-dimensional tensor. Any of these matrices or tensors always sum to the amount of possible start states, as the sum of probabilities of possible transitions from any state $j$ is $1$. On each axis of these matrices or tensors we find each possible state (here: $\mathcal{S} = \{a,c,p\}$), so that the size of this matrix/tensor is $\mathcal{R}^{{\left\lVert S \right\rVert}^{T+1}}$ (where $T$ is the order of the model).

Similar to the initial state probabilities, we define a matrix for all $\phi_2(j)$, and a tensor for all $\phi_3(j)$ (as we are only evaluating 1st- and 2nd-order models). $\phi_3(j)$ is then used as $\phi_t(j)$ in 2nd-order models (and similarly, $\phi_2(j)$ is used as $\phi_t(j)$ in 1st-order models).

As a convention, the dimensions in these matrices and tensors are ordered from most recent to oldest, i.e., $\boldsymbol A_{t_0,t_{-1}, \dots, t_{-T}}$. This means that we can query similar to ".. what is the probability of $t_0$, given that we were in $t_{-1}$ before and $t_{-2}$ befor that?" using that notion.

```{r}
transprobs_1stOrder <- matrix(data = 0, nrow = 3, ncol = 3)
colnames(transprobs_1stOrder) <- levels(commits_t1$label_t_0)
rownames(transprobs_1stOrder) <- levels(commits_t1$label_t_0)

for (t_1 in levels(commits_t1$label_t_1)) { # column-wise
  for (t_0 in levels(commits_t1$label_t_1)) {
    # Sum how often we went from t_1 to t_0
    transprobs_1stOrder[t_0, t_1] <- transprobs_1stOrder[t_0, t_1] +
      sum(commits_t1$label_t_1 == t_1 & commits_t1$label_t_0 == t_0)
  }
  
  # Normalize all options for ending up in t_0 coming from t_1:
  transprobs_1stOrder[, t_1] <- transprobs_1stOrder[, t_1] /
    sum(transprobs_1stOrder[, t_1])
}

print(transprobs_1stOrder)
```

As an example, to go over `p` to `a` (or to end up in `a` having gone over `p`), we select the transition probability as `r transprobs_1stOrder["a", "p"]`. For any higher-dimension tensors, we prepend dimensions, so that we can follow this scheme (going over .. to ..).

### Transition Tensor for 2nd-order Models
We do this in an extra section as we will work with actual tensors and the initialization is a bit different. We stick to the same indexing convention as for 2D-matrices.

```{r}
install.packagesCond("tensorr")
library("tensorr")

# Create a dense 3x3x3 tensor
transprobs_2ndOrder <- dtensor(array(data = 0, dim = c(3,3,3)))
dimnames(transprobs_2ndOrder) <-
  list(levels(commits_t0$label_t_0), levels(commits_t0$label_t_0), levels(commits_t0$label_t_0))

# Now let's fill the tensor using a numeric mapping a=1, c=2, p=3:
m <- c("a" = 1, "c" = 2, "p" = 3)
for (t_2 in levels(commits_t2$label_t_2)) {
  i2 <- m[t_2]
  for (t_1 in levels(commits_t2$label_t_1)) {
    i1 <- m[t_1]
    for (t_0 in levels(commits_t2$label_t_0)) {
      i0 <- m[t_0]
      
      # Sum how often we went from t_2, over t_1, to t_0
      transprobs_2ndOrder[i0, i1, i2] <- transprobs_2ndOrder[i0, i1, i2] +
        sum(commits_t2$label_t_2 == t_2 &
            commits_t2$label_t_1 == t_1 &
            commits_t2$label_t_0 == t_0)
    }
  }
  
  # Normalize each 3x3x1 tensor:
  n <- transprobs_2ndOrder[,, i2] / sum(transprobs_2ndOrder[,, i2])
  transprobs_2ndOrder[,, i2] <- array(n, dim = dim(n))
}

#transprobs_2ndOrder <- transprobs_2ndOrder / sum(transprobs_2ndOrder)
print(transprobs_2ndOrder)
```

As an example, to go from `p` to `c` and then `a`, we call `transprobs_2ndOrder[m["a"], m["c"], m["p"]]`, and the probability is `r transprobs_2ndOrder[m["a"], m["c"], m["p"]]`. We have to use the `m[label]`-notation, as indexing of dimensions does not work on other dimensions other than the last for some reason.



# Model Evaluation
We want to evaluate *depmix*- and *conddens*-models and check their classification accuracy. However, we can separate all tests to those that require consecutive commits (chains with proper relations) for inferencing, and those that do not.

Models that can be evaluated as stateless classifiers:

* condDens models B and C
* models supported by `mmb`
* other models, such as Random forest

Models that require consecutive observations for evaluation:

* all depmix models
* condDens model A


For stateless models, we will do a classic cross-validation. For stateful models, we will train using all but one of the available chains of observations, and then inference on that remaining chain.

We will only train and evaluate cross-project models (mainly because we don't have much data).


## Most important Variables

Furthermore, we are applying Recursive Feature Elimination (RFE) to find the most important features. We search among the `t_0`-features and will assume that any `t_n`-features of preceding commits have a comparable impact on classification performance.

We will be doing an RFE using the `commit_t0` dataset. Across all datasets of commits we ignore these columns:

```{r}
# Mostly columns that could be used to identify a column. Note that
# we keep "label", as it'll be removed for inferencing only.
ignoreCols <- c(
  "project", # because of scarce data, we train and evaluate only cross-project
  "commitId",
  "labelledBefore",
  "reasonOrRule",
  "RepoPathOrUrl",
  "CommitterName",
  "CommitterTime",
  "CommitterEmail",
  "CommitterNominalLabel",
  "ParentCommitSHA1s",
  "Message",
  "branch",
  "SHA1",
  "AuthorName",
  "AuthorTime",
  "AuthorEmail",
  "AuthorNominalLabel"
)
```

Let's do the RFE:

```{r}
install.packagesCond("doParallel")
library(doParallel)
install.packagesCond("caret")
library(caret)
install.packagesCond("mlbench")
library(mlbench)
install.packagesCond("randomForest")
library(randomForest)


resultsRfeFile <- "../results/resultsRfe.rds"
if (file.exists(resultsRfeFile)) {
  resultsRfe <- readRDS(resultsRfeFile)
  resultsRfe$X <- NULL
} else {
  resultsRfe <- doRfe(
    data = commits_t0[, !grepAnyOrAll(ignoreCols, cn)],
    y = commits_t0$label_t_0,
    yColName = "label_t_0")
  saveRDS(resultsRfe, resultsRfeFile)
}

plot(resultsRfe, type = c("g", "o"))
print(caret::predictors(resultsRfe))
```


## Stateless Models
For each dataset (`commits_t0`, `commits_t1`, `commits_t2`, `commits_t2`) we do a separate RFE, and then we do the following:

* Re-order the dataset's columns by importance of the features.
* For all 3rd-parts models we train using a repeated cross validation, using caret and a split of `0.8`.
* For conditional-density models, we keep the same kind of variables in every generation and perform also a cross validation. We use the same split.
  * Note we cannot use `t0`-datasets for these models, as there are no relations. However, we can compare our first- and second-order models' performance to the 3rd-party models that use `t0`-data.
  * Note that only model __B__ supports a __3rd__-order.
  
### Computation of 3rd-Party Models
Since we have transformed our problem into one suitable for stateless classifiers, we will attempt using some of them so that we can compare their results and performance later.

The 3rd-party models are these:

```{r warning=FALSE}
install.packagesCond("randomForest")
install.packagesCond("e1071")
install.packagesCond("ranger")
install.packagesCond("dplyr")
install.packagesCond("plyr")
install.packagesCond("gbm")
install.packagesCond("naivebayes")
install.packagesCond("C50")

# bayesCaret refers to mmb-models!
caret_models <- c(#"bayesCaret")#, 
  "null", "rf", "ranger", "gbm", "naive_bayes", "C5.0")
```

Let's do the computation!

```{r}
stateless_folds = 5
stateless_repeats = 3
results_stateless <- data.frame()


resFile <- paste0("../results/stateless.csv")
if (file.exists(resFile)) {
  temp <- read.csv(resFile)
  temp$X <- NULL
  results_stateless <- rbind(results_stateless, temp)
} else {
  doWithParallelCluster({
    for (idx in c(0:3)) {
      yColName <- paste0("label_t_", idx)
      data <- .GlobalEnv[[paste0("commits_t", idx)]]
      # Compute it!
      rfe <- doRfe(
        data = data,
        y = data[[yColName]],
        yColName = "^label_t_",
        ignoreCols = ignoreCols,
        number = stateless_folds, repeats = stateless_repeats)
      
      useVars <- predictors(rfe)
      plot(rfe, type = c("g", "o"))
      
      # Let's reorder the dataset:
      data <- data[, c(yColName, useVars)]
      
      for (modelName in caret_models) {
        trainedModel <- evaluateStatelessModel(
          ds = data, modelName = modelName, yColName = yColName, trP = 0.8,
          rcvNumber = stateless_folds, rcvRepeats = stateless_repeats)
        result <- trainedModel$results[which.max(trainedModel$results$Accuracy), ]
        
        results_stateless <- rbind(results_stateless, data.frame(
          vars = length(useVars),
          t = idx,
          model = modelName,
          acc = result$Accuracy,
          kappa = result$Kappa
        ))
      }
    }
  })
  
  write.csv(results_stateless, resFile)
}

print(results_stateless)
```


### Computation of conditional Density Models

We will train the conditional density models separately, because we do not have a caret-compatible wrapper at this moment. Caret did an n-repeated, k-fold cross validation with all the data, using a split of `0.8`. We will do the same, n*k times, withholding 20% of the data at every turn. Then we predict it and average the results. Additionally, we will do this for the `doEcdf`-parameter.


```{r}
perms <- rbind(
  expand.grid(
    t = 1:3,
    doEcdf = c(TRUE, FALSE),
    rfeVars = c(TRUE, FALSE), # Use RFE-filtered variables or ALL
    resample = 1:(stateless_folds * stateless_repeats),
    model = c("B1", "B2", "B3")
  ),
  expand.grid(
    t = 1:2,
    doEcdf = c(TRUE, FALSE),
    rfeVars = c(TRUE, FALSE),
    resample = 1:(stateless_folds * stateless_repeats),
    model = c("C1", "C2")
  )
)

```

```{r}
permsRfe <- doWithParallelCluster({
  foreach::foreach(
    indexes = 1:3,
    .packages = c("parallel", "doParallel")
  ) %dopar% {
    clNested <- makePSOCKcluster(max(2, detectCores() / 2))
    registerDoParallel(clNested)
    idx <- indexes
    
    yColName <- paste0("label_t_", idx)
    data <- if (idx == 1) commits_t1 else if (idx == 2) commits_t2 else commits_t3

    # Compute it!
    temp <- doRfe(
      data = data,
      y = data[[yColName]],
      yColName = "^label_t_",
      ignoreCols = ignoreCols,
      number = stateless_folds, repeats = stateless_repeats, maxSize = 100)
    
    stopCluster(clNested)
    temp
  }
})
```

Let's show those RFE plots:
```{r}
for (n in 1:length(permsRfe)) {
  plot(permsRfe[[n]], type=c("o","g"))
}
```



```{r}
results_stateless_conddens <- data.frame()
resFileConddens <- paste0("../results/stateless_cd.csv")
if (file.exists(resFileConddens)) {
  temp <- read.csv(resFileConddens)
  temp$X <- NULL
  results_stateless_conddens <- temp
} else {
  results_stateless_conddens <- doWithParallelCluster({
    foreach::foreach(
      permRowIdx = rownames(perms),
      .combine = rbind,
      .packages = c("caret")
    ) %dopar% {
      permRow <- perms[permRowIdx, ]
      permRfe <- permsRfe[[permRow$t]]
      
      t <- permRow$t
      doEcdf <- permRow$doEcdf
      rfeVars <- permRow$rfeVars
      resamp <- permRow$resample
      mType <- permRow$model
      
      model <- if (mType == "B1") condDens_B_1stOrder else if
      (mType == "B2") condDens_B_2ndOrder else if
      (mType == "B3") condDens_B_3rdOrder else if
      (mType == "C1") condDens_C_1stOrder else if
      (mType == "C2") condDens_C_2ndOrder else stop(paste("Model not known:", mType))
      
      data <- if (t == 1) commits_t1 else if (t == 2) commits_t2 else commits_t3
      
      # Use all variables for estimating densities and probabilities
      # or just those left after RFE?
      useVars <- if (rfeVars) predictors(permRfe) else colnames(data)
    
      # Take all variables used by rfe, in each generation.
      # Note that these will not be ordered (which is not required)
      # We need to include the labels for our models so they can
      # build conditional densities.
      temp <- c("label_t_", "MinutesSince", unique(gsub("_t_\\d+$", "", useVars)))
      data <- data[, colnames(data)[grepAnyOrAll(
        patterns = temp, subjects = colnames(data))]]
      
      # Split data to train and validation:
      set.seed(t * resamp)
      p <- caret::createDataPartition(data[["label_t_0"]], p = 0.8, list = FALSE)
      train <- data[p, ]
      valid <- data[-p, ]
      
      truth <- valid[, "label_t_0"]
      pred <- model(
        states = states,
        data = train,
        observations = valid,
        stateColumn = "label_t_0",
        doEcdf = doEcdf)
      
      cm <- caret::confusionMatrix(truth, factor(pred, levels = levels(truth)))
      
      cbind(permRow, data.frame(
        vars = length(temp),
        acc = cm$overall[[1]],
        kappa = cm$overall[[2]]
      ))
    }
  })
  
  write.csv(results_stateless_conddens, resFileConddens)
}
```

Todo: We need to add some plots here later!


## Stateful Models
In this section, we dedicate some effort into testing the Dependent Mixture Models (depmix), as well as some stateful models that facilitate conditional density.

We have a number of consecutively labeled chains of commits. Since stateful models require going from one commit to the next, we will attempt inferencing each chain individually. The chains are differently long and belong to different projects, so we have some kind of cross validation.

First we get all chains. Then, for each change, we will take the entire dataset and splice out the commits on the chain, thereby separating into training and validation data.

```{r}
treeXL <- getDataset("rq5_treex", removeUnwantedColums = FALSE)
treeXChains <- buildObservationChains(
  data = treeXL, idCol = "commitId", parentIdCol = "ParentCommitSHA1s")
```

```{r}
permsStateful <- rbind(
  expand.grid(
    t = 1,
    doEcdf = c(TRUE, FALSE),
    rfeVars = c(TRUE, FALSE), # Use RFE-filtered variables or ALL
    resample = 1:(stateless_folds * stateless_repeats),
    model = c("A1", "depmix1")
  ),
  expand.grid(
    t = 2,
    doEcdf = c(TRUE, FALSE),
    rfeVars = c(TRUE, FALSE), # Use RFE-filtered variables or ALL
    resample = 1:(stateless_folds * stateless_repeats),
    model = c("A2", "depmix2")
  )
)
```
```{r}
results_statefull <- data.frame()
resFileStatefull <- paste0("../results/statefull.csv")
if (file.exists(resFileStatefull)) {
  temp <- read.csv(resFileStatefull)
  temp$X <- NULL
  results_statefull <- temp
} else {
  results_statefull <- doWithParallelCluster({
    foreach::foreach(
      permRowIdx = rownames(permsStateful),
      .combine = rbind,
      .packages = c("caret")
    ) %dopar% {
      permRow <- permsStateful[permRowIdx, ]
      permRfe <- permsRfe[[permRow$t]]
      
      t <- permRow$t
      doEcdf <- permRow$doEcdf
      rfeVars <- permRow$rfeVars
      resamp <- permRow$resample
      mType <- permRow$model
      
      tryCatch({
        
        model <- if (mType == "A1") condDens_A_1stOrder else if
        (mType == "A2") condDens_A_2ndOrder else if
        (mType == "depmix1") depmixForward_1stOrder else if
        (mType == "depmix2") depmixForward_2ndOrder else stop(paste("Model not known:", mType))
        
        data <- if (t == 1) commits_t1[, ] else commits_t2[, ]
        lvl <- levels(data[["label_t_0"]])
        data[["label_t_0"]] <- as.character(data[["label_t_0"]])
        
        # Use all variables for estimating densities and probabilities
        # or just those left after RFE?
        useVars <- if (rfeVars) predictors(permRfe) else colnames(data)
        
        
        # We should take 0.8 of the data for training, and
        # the rest for inferencing. However, we can only take
        # data chain-wise. So we'll start and inferencing chain
        # by chain and stop once we have at least taken 0.2%.
        # We'll take random chains.
        
        set.seed(t * resamp)
        chainNames <- sample(names(treeXChains))
  
        dataLen <- nrow(data)
        pred <- c()
        truth <- c()
        
        chainIdx <- 1
        
        while (TRUE) {
          if (nrow(data) <= dataLen * 0.8) {
            break
          }
          
          nextChain <- chainNames[chainIdx]
          
          ## Now instead of train/validation, we have all commits
          ## but those on the currently selected chain.
          trainValid <- extractChain(
            data = data, idColData = "commitId_t_0", idColChain = "commitId",
            t = t, chains = treeXChains, chainId = nextChain)
          train <- trainValid$data
          valid <- trainValid$obs
          
          data <- trainValid$data
          chainIdx <- chainIdx + 1
          
          # Take all variables used by rfe, in each generation.
          # Note that these will not be ordered (which is not required)
          # We need to include the labels for our models so they can
          # build conditional densities.
          temp <- c("label_t_", "MinutesSince", unique(gsub("_t_\\d+$", "", useVars)))
          train <- train[, colnames(train)[grepAnyOrAll(
            patterns = temp, subjects = colnames(train))]]
          valid <- valid[, colnames(valid)[grepAnyOrAll(
            patterns = temp, subjects = colnames(valid))]]
          
          truth <- c(truth, valid[, "label_t_0"])
          # Remove the label from valid, to avoid the special cases
          # for depmix models.
          valid[["label_t_0"]] <- ""
          
          pred <- c(pred, model(
            states = states,
            data = train,
            observations = valid,
            stateColumn = "label_t_0",
            doEcdf = doEcdf))
        }
        
        truth <- factor(truth, levels = lvl)
        pred <- factor(pred, levels = lvl)
        cm <- caret::confusionMatrix(truth, pred)
        
        cbind(permRow, data.frame(
          vars = length(temp),
          acc = cm$overall[[1]],
          kappa = cm$overall[[2]],
          numChains = chainIdx - 1,
          nrowData = nrow(data),
          truth = paste(truth, collapse = ","),
          pred = paste(pred, collapse = ",")
        ))
      }, error=function(cond) {
        stop(paste(t, doEcdf, rfeVars, resamp, mType, cond))
      })
    }
  })
  
  write.csv(results_statefull, resFileStatefull)
}

    
```


### Pseudo-stateful 3rd-party Models

In this section, we create the data as it is used internally by the joint conditional density models that segment over one or more generations of parents and estimate densities over the current generation's variables.

Here however, we do this only by segmenting over the previous generation. For each label, we train a separate model. Later, we combine the votes of these models by averaging.

```{r}
install.packagesCond("neuralnet")
library(neuralnet)

set.seed(847321)
p <- caret::createDataPartition(commits_t1[["label_t_0"]], p = 0.8, list = FALSE)
train <- commits_t1[p, ]
valid <- commits_t1[-p, ]

# We'll store the neural nets in this list.
# We need one model per label.
prevModels <- list()
lvls <- levels(commits_t1$label_t_0)
for (lvl in lvls) {
  tempData <- train[train$label_t_1 == lvl, ]
  tempData <- tempData[, !grepAnyOrAll(c(ignoreCols, "t_1$"), colnames(tempData))]
  
  # We need those for one-hot encoding:
  tempData$isA <- sapply(tempData$label_t_0, function(l) if (l == "a") 1 else 0)
  tempData$isC <- sapply(tempData$label_t_0, function(l) if (l == "c") 1 else 0)
  tempData$isP <- sapply(tempData$label_t_0, function(l) if (l == "p") 1 else 0)
  # Remove the label (we use the one-hot now):
  tempData$label_t_0 <- NULL
  
  set.seed(1337)
  tempNet <- neuralnet::neuralnet(
    formula = isA + isC + isP ~.,
    data = tempData, hidden = c(3))
  
  prevModels[[lvl]] <- tempNet
}

# Prepare the validation data, too:
validGroundTruth <- sapply(valid$label_t_0, function(l) {
  if (l == "a") return(1)
  if (l == "c") return(2)
  return(3)
})

valid <- valid[, !grepAnyOrAll(c(ignoreCols, "t_1$"), colnames(valid))]
valid$label_t_0 <- NULL
```

Let's do some inferencing. Note that, depending on the random seed, the train and validation sets differ greatly, and the following inferencing typically results in ~55% accuracy, with the lowest about 45%, and the highest around 63% (Kappa 0.3). This is still a bit lower compared with where the other 3rd-party models max out.

```{r}
# We'll combine predictions by adding them together, then returning
# which output was max.

predTempA <- predict(prevModels[["a"]], newdata = valid)
predTempC <- predict(prevModels[["c"]], newdata = valid)
predTempP <- predict(prevModels[["p"]], newdata = valid)

predTemp <- predTempA + predTempC + predTempP
predTruth <- unname(apply(predTemp, 1, which.max))

caret::confusionMatrix(
  factor(predTruth, levels = c(1,2,3)),
  factor(validGroundTruth, levels = c(1,2,3)))
```

Let's do the same, but this time we take the maximum value across all three models instead of averaging:

```{r}
predTemp <- cbind(predTempA, predTempC, predTempP)
predTruth <- unname(apply(predTemp, 1, function(row) {
  m <- which.max(row) %% 3
  return(if (m == 0) 3 else m)
}))

caret::confusionMatrix(
  factor(predTruth, levels = c(1,2,3)),
  factor(validGroundTruth, levels = c(1,2,3)))
```


### Additional approaches for pseudo-stateful 3rd-party models

This did not work too well. However, we could try an additional approach close to $\boldsymbol e_{\boldsymbol i,j}$-models (1st-order only), where we build number of labels to the power of the order + 1 models (here: 9 models for 1st-order).

The idea is the following:

For each label and transition (e.g., going from "c" to "a"), we build a model over all $t_0$-commits. The data would become scarce quickly, as it would mean segmenting over two variables. To counter this, we suggest killing two birds with one stone:

* Partition the commits into those that correspond to the given transition, and those that do not.
* Keep a few of those that correspond as validation data and withhold entirely.
* Create a new binary (yet numeric) label with $1$ for the first set of commits, and $0$ for all others. That way, we get a lot of training data for each model.
* Since we only regress to one value, we can use almost any of the 3rd-party models, such as Random forest. Also, having standard 3rd-party models, each model can be tuned separately, or we can even combine them using, e.g., stacking or boosting.

In our case, we get 3 models each leading to each type of commit (e.g., there are 3 models that go from one of a/c/p to a). When inferencing, we have a number of options for achieve a vote, e.g.:

* Take the maximum or average/sum from each group of models.
* Same, but take max/avg from each individual model
* ...


### Check voting-mechanism of $e_{i,j}$-models

Currently, for the $\boldsymbol e_{\boldsymbol i,j}$-likelihood estimators, we add together the sums of likelihoods for all estimators that end in state $j$, and then we take the max as prediction. However, it may be advisable to take the sole maximum across all individual estimators instead. The voting-scheme for other models should also be checked.


### Add properties of previous observations

As an example, we can extend our $\boldsymbol e_{\boldsymbol i,j}$-likelihood estimators to also include densities over the observed properties of the preceding observation. This could lead to a greater spread in higher and lower likelihoods and be a tie-breaker. In other words, if selecting the correct predecessor too, then this may result in higher likelihoods if the $j$-th observation is chosen correctly. Note that this would require a change in the voting-scheme, because now we would need to select the estimator with highest maximum likelihood (i.e., no grouping).


### Re-run tests with normalized PDFs

It might be that using unnormalized PDFs across estimators as we have done it so far is not correct, as different estimators use different (and a different amount of) underlying observations, leading to inhomogeneous domains. We should probably re-run all models using `normalizePdf=TRUE`.


### Adding trainable parameters to Joint Conditional Density models

The JCD models currently add up the relative likelihoods for each feature's value, and there is no weighting. However, instead of adding trainable parameters to these models, we suggest using a neural network as 2nd stage like this:

* Normalize the output into the range $[0,1]$ of each estimated density function. This can be easily done by dividing the result by $y_{max}$, which is part of the estimated function. The probability masses for discrete features are already in this range.
* Use these transformed values as input for a neural network, which itself uses the _Swish_-activation function, and _one-hot_-encoding or just a binary output (see below).
* Note that we may have to train a network per estimator or groups of estimators (see below). Then, the votes may be combined or simply the maximum is taken.

The way a neural network may be operationalized depends on the type of JCD. In the case of our 1st-order $\boldsymbol C$-models (which use the $\boldsymbol e_{\boldsymbol i,j}$-likelihood estimators), we have already number of labels squared estimators. Also, as input-observation, there is no information about the preceding observation. This means that we probably need to aggregate all $\boldsymbol i$-estimators, as we only have information about $j$ (being the current observation). It may also be worth testing to use ___all___ estimators and have a one-hot output encoding.

```{r}
set.seed(847321)
p <- caret::createDataPartition(commits_t1[["label_t_0"]], p = 0.8, list = FALSE)
tempData <- commits_t1[, !grepAnyOrAll(
  c(ignoreCols, "t_1$"), colnames(commits_t1))]
train <- tempData[p, ]
valid <- tempData[-p, ]

pred <- condDens_C_1stOrder_weighted(
  states = states, data = train, observations = valid, stateColumn = "label_t_0")
```

```{r}
temp <- pred[pred[,1]==1,]
bla <- c()
for (i in 0:((nrow(temp)/3))-1) {
  tempMat <- temp[(i*3+1):(i*3+3), states]
  bla <- c(bla, states[which(tempMat == max(tempMat), arr.ind = TRUE)[1, "row"]])
}
bla <- factor(x=bla[2:length(bla)], levels = states)

caret::confusionMatrix(bla, valid$label_t_0)
```


### Improve previous test-case

The results of the previous approach are terrible. However, we have many degrees of freedom and many ways this could have gone wrong.

I do suggest the following two steps:

1. Extend $e_{i,j}$-models to include the densities of the previous observation(s) (but not their label). This is more closely described in _Add properties of previous observations_.
  * Then just re-test these models, as I would definitely expect an improvement of the results.
2. Add neural networks to these models.
  * The previous extension is crucial, because it significantly changes the voting-scheme and avoids grouping of estimators as done previously.
  * Now, each estimator gets its own separate 2nd stage.

More details for "neural afterburner":

* The following assumptions are for 1st-order models based on the $e_{i,j}$-estimators but can easily extended to the 2nd- or nth-order. Our earlier work has shown that taking 3, 5 or more previous observations is actually of value sometimes.
* Each partial estimator should get a one-hot encoding. For three labels and 1st-order estimators, this will result in 6 output-neurons; 2nd-order would be 9 already.
* Each observation passed through each pipeline, that is, transforming it to likelihoods using the current $e_{i,j}$-model's densities, then passing that result through the associated neural network to get the activations.
* The training for each estimator's neural network would use all the training data, labelled accordingly. Example: The estimator for the sequence $a,a$ would activate as $1,0,0,1,0,0$ for such an $a,a$-observation. An estimator $c,a$ would activate as $0,0,0,1,0,0$, because the observation itself would be correct, but not its predecessor. So the estimator for $p,c$ would activate with zeros only.

__NOTE__: The above is not entirely correct, and there is an extensive audio-recording explaining the process.











